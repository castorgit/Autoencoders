{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE real data electricity latent space 16 real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Autoencoders for non-supervised intrusion detection\n",
    "\n",
    "Normal Data : Real Data <br>\n",
    "Attack Data : Real Data <br>\n",
    "1000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 12:14:45.735909: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-12 12:14:45.735929: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Lambda, Input, Dense\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy, kl_divergence\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_marker(row):\n",
    "    if isinstance(row['marker'], str) and 'Natural' in row['marker']:\n",
    "        return 0\n",
    "    if isinstance(row['marker'], str) and 'Attack' in row['marker']:\n",
    "        return 1\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_figures = './figures/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "55663\n"
     ]
    }
   ],
   "source": [
    "#path = './data/Zero_Day/'\n",
    "path = './new_data/'\n",
    "path_figures = './figures/'\n",
    "path_normal = path +'VAE_syntheticdata_normal_epoch100_1000000.csv'\n",
    "#path_normal = path +'normal.csv'\n",
    "path_attack = path + 'attack.csv' # real data\n",
    "\n",
    "normal_df = pd.read_csv(path_normal)  \n",
    "attack_df = pd.read_csv(path_attack)  \n",
    "\n",
    "normal_df['marker'] = normal_df.apply(label_marker, axis=1)\n",
    "attack_df['marker'] = attack_df.apply(label_marker, axis=1)\n",
    "\n",
    "normal_label = normal_df['marker'].to_numpy() \n",
    "attack_label = attack_df['marker'].to_numpy() \n",
    "\n",
    "print(len(normal_label))\n",
    "print(len(attack_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete marker column \n",
    "normal_df.drop(['marker'], axis=1, inplace=True)\n",
    "attack_df.drop(['marker'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal shape (1000000, 128)\n",
      "attack shape (55663, 128)\n"
     ]
    }
   ],
   "source": [
    "# each row of the dataframe is considered a sequence\n",
    "# we create a numpy array with the sequences\n",
    "normal_np = normal_df.to_numpy()\n",
    "print('normal shape',normal_np.shape)\n",
    "attack_np = attack_df.to_numpy()\n",
    "print('attack shape', attack_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df = normal_df.replace([np.inf, -np.inf], 0)\n",
    "scaler = MinMaxScaler()\n",
    "normal_scaled = scaler.fit_transform(normal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_df = attack_df.replace([np.inf, -np.inf], 0)\n",
    "scaler = MinMaxScaler()\n",
    "attack_scaled = scaler.fit_transform(attack_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = combine_labels\n",
    "#norm_samples = df_scaled[labels == 0]  # normal data\n",
    "#attack_samples = df_scaled[labels == 1]  # attack data\n",
    "\n",
    "#norm_labels = labels[labels == 0]\n",
    "#attack_labels = labels[labels == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55663, 128)\n",
      "(1000000, 128)\n"
     ]
    }
   ],
   "source": [
    "norm_samples = normal_scaled\n",
    "attack_samples = attack_scaled\n",
    "print (attack_samples.shape)\n",
    "print (norm_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate train set\n",
    "# training set will consist of the normal ds\n",
    "\n",
    "len_norm = len(norm_samples)\n",
    "len_norm_train = int(0.80 * len_norm)\n",
    "X_train = norm_samples[:len_norm_train]\n",
    "\n",
    "# generate test set consist of 50% attack and 50% normal\n",
    "\n",
    "X_test_norm = norm_samples[len_norm_train:]\n",
    "len_attack_test = len(X_test_norm) # we will use the same number\n",
    "X_test_attack = attack_scaled[:len_attack_test]\n",
    "\n",
    "X_test = np.concatenate([X_test_norm, X_test_attack])\n",
    "y_test = np.ones(len(X_test))\n",
    "y_test[:len(X_test_norm)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X_test normal shape (200000, 128) \n",
      " X_test attack shape (55663, 128) \n",
      " X_train normal shape (800000, 128) \n",
      " X_test shape (255663, 128)\n"
     ]
    }
   ],
   "source": [
    "X_train.shape\n",
    "print(' X_test normal shape',X_test_norm.shape, '\\n', \n",
    "      'X_test attack shape', X_test_attack.shape, '\\n',\n",
    "      'X_train normal shape', X_train.shape, '\\n',\n",
    "      'X_test shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use RMSE as reconstruction error (can use MAE as well)\n",
    "def get_error_term(v1, v2, _rmse=True):\n",
    "    if _rmse:\n",
    "        return np.sqrt(np.mean((v1 - v2) ** 2, axis=1))\n",
    "    #return MAE\n",
    "    return np.mean(abs(v1 - v2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reparameterization trick for the autoencoder\n",
    "\n",
    "def sample(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " original_dim :  128 \n",
      " intermediate_dim :  64 \n",
      " latent_dim :  16\n"
     ]
    }
   ],
   "source": [
    "original_dim = X_train.shape[1]\n",
    "input_shape = (original_dim,)\n",
    "intermediate_dim = int(original_dim / 2) \n",
    "latent_dim = int(original_dim / 3) \n",
    "\n",
    "#intermediate_dim = 256\n",
    "latent_dim = 16\n",
    "print(' original_dim : ', original_dim,'\\n',\n",
    "      'intermediate_dim : ', intermediate_dim, '\\n', \n",
    "      'latent_dim : ', latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           8256        ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           2080        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 16)           528         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 16)           528         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " z (Lambda)                     (None, 16)           0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,392\n",
      "Trainable params: 11,392\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# enconder >< decoder\n",
    "# encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x      = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "x      = Dense(intermediate_dim/2, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "# use the reparameterization trick and get the output from the sample() function\n",
    "z        = Lambda(sample, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "encoder  = Model(inputs, z, name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " z_sampling (InputLayer)     [(None, 16)]              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,976\n",
      "Trainable params: 10,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x             = Dense(intermediate_dim/2, activation='relu')(latent_inputs)\n",
    "x             = Dense(intermediate_dim, activation='relu')(x)\n",
    "outputs       = Dense(original_dim, activation='sigmoid')(x)\n",
    "# Instantiate the decoder model:\n",
    "decoder       = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full VAE model\n",
    "outputs   = decoder(encoder(inputs))\n",
    "vae_model = Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the KL loss function:\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    # compute the average MSE error, then scale it up, ie. simply sum on all axes\n",
    "    reconstruction_loss = K.sum(K.square(x - x_decoded_mean))\n",
    "    # compute the KL loss\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.square(K.exp(z_log_var)), axis=-1)\n",
    "    # return the average loss over all \n",
    "    total_loss = K.mean(reconstruction_loss + kl_loss)    \n",
    "    #total_loss = reconstruction_loss + kl_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae_mlp\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 128)]             0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 16)                11392     \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 128)               10976     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,368\n",
      "Trainable params: 22,368\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 800000 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 12:15:23.435734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-12 12:15:23.436051: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-12 12:15:23.436113: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-12 12:15:23.436182: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-12 12:15:23.437907: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-12 12:15:23.437971: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-12 12:15:23.438028: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-10-12 12:15:23.438037: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-10-12 12:15:23.438600: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000/800000 [==============================] - 30s 37us/sample - loss: 96.3113\n",
      "Epoch 2/100\n",
      "800000/800000 [==============================] - 30s 38us/sample - loss: 83.5738\n",
      "Epoch 3/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 81.1560\n",
      "Epoch 4/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 79.3762\n",
      "Epoch 5/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 78.3964\n",
      "Epoch 6/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 77.4481\n",
      "Epoch 7/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 76.6561\n",
      "Epoch 8/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 76.2352\n",
      "Epoch 9/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 75.9670\n",
      "Epoch 10/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 75.7524\n",
      "Epoch 11/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 75.5793\n",
      "Epoch 12/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 75.4541\n",
      "Epoch 13/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 75.3535\n",
      "Epoch 14/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 75.2611\n",
      "Epoch 15/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 75.1560\n",
      "Epoch 16/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 75.0051\n",
      "Epoch 17/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 74.8173\n",
      "Epoch 18/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 74.6831\n",
      "Epoch 19/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 74.5812\n",
      "Epoch 20/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 74.5057\n",
      "Epoch 21/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 74.4542\n",
      "Epoch 22/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 74.4175\n",
      "Epoch 23/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 74.3820\n",
      "Epoch 24/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 74.3517\n",
      "Epoch 25/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 74.3131\n",
      "Epoch 26/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 74.2711\n",
      "Epoch 27/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 74.2289\n",
      "Epoch 28/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 74.1716\n",
      "Epoch 29/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 74.0933\n",
      "Epoch 30/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 74.0114\n",
      "Epoch 31/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.9084\n",
      "Epoch 32/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.7997\n",
      "Epoch 33/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.7260\n",
      "Epoch 34/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.6641\n",
      "Epoch 35/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.6275\n",
      "Epoch 36/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.5890\n",
      "Epoch 37/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.5709\n",
      "Epoch 38/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.5403\n",
      "Epoch 39/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.5214\n",
      "Epoch 40/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.5083\n",
      "Epoch 41/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4899\n",
      "Epoch 42/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4708\n",
      "Epoch 43/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4593\n",
      "Epoch 44/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4538\n",
      "Epoch 45/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4573\n",
      "Epoch 46/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4527\n",
      "Epoch 47/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4441\n",
      "Epoch 48/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4371\n",
      "Epoch 49/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4366\n",
      "Epoch 50/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4281\n",
      "Epoch 51/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4246\n",
      "Epoch 52/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4247\n",
      "Epoch 53/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4278\n",
      "Epoch 54/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4295\n",
      "Epoch 55/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4296\n",
      "Epoch 56/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4276\n",
      "Epoch 57/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4319\n",
      "Epoch 58/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4269\n",
      "Epoch 59/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4295\n",
      "Epoch 60/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4163\n",
      "Epoch 61/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4240\n",
      "Epoch 62/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4270\n",
      "Epoch 63/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4391\n",
      "Epoch 64/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4487\n",
      "Epoch 65/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4584\n",
      "Epoch 66/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4835\n",
      "Epoch 67/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4784\n",
      "Epoch 68/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.5001\n",
      "Epoch 69/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.4992\n",
      "Epoch 70/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.5051\n",
      "Epoch 71/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.5257\n",
      "Epoch 72/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.5533\n",
      "Epoch 73/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.5616\n",
      "Epoch 74/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.5727\n",
      "Epoch 75/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.5686\n",
      "Epoch 76/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.5702\n",
      "Epoch 77/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.5865\n",
      "Epoch 78/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.5848\n",
      "Epoch 79/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.5953\n",
      "Epoch 80/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.5884\n",
      "Epoch 81/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.6023\n",
      "Epoch 82/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.6080\n",
      "Epoch 83/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.6134\n",
      "Epoch 84/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.6336\n",
      "Epoch 85/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.6475\n",
      "Epoch 86/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.6498\n",
      "Epoch 87/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.6587\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.6728\n",
      "Epoch 89/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.6718\n",
      "Epoch 90/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.7114\n",
      "Epoch 91/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.7200\n",
      "Epoch 92/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.7263\n",
      "Epoch 93/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.7145\n",
      "Epoch 94/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.7135\n",
      "Epoch 95/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.7348\n",
      "Epoch 96/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 73.7127\n",
      "Epoch 97/100\n",
      "800000/800000 [==============================] - 31s 38us/sample - loss: 73.7241\n",
      "Epoch 98/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.7398\n",
      "Epoch 99/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.7229\n",
      "Epoch 100/100\n",
      "800000/800000 [==============================] - 31s 39us/sample - loss: 73.6910\n"
     ]
    }
   ],
   "source": [
    "# Training autoencoder\n",
    "opt = optimizers.Adam(learning_rate=0.0001, clipvalue=0.5)\n",
    "\n",
    "vae_model.compile(optimizer=opt, loss=vae_loss)\n",
    "vae_model.summary()\n",
    "\n",
    "results = vae_model.fit(X_train, X_train,\n",
    "                        shuffle=True,\n",
    "                        epochs=100,\n",
    "                        batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAhz0lEQVR4nO3de5RU5Znv8e9T1753Q9MgNCCgBi8oGNBgvMRRkxMxjvfLmegxjqOZWWbFZCWZo8nMycycc5LMOebkMolJcDRxkmguXqJJjImaUZOFN0RUFBVBkAaEpqVpuulLXZ7zx97dNNhgN1AUvffvsxarqnbtXfW8dPdvv/Xut/Y2d0dEROIjUe4CRETkwFLwi4jEjIJfRCRmFPwiIjGj4BcRiRkFv4hIzCj4RfbAzH5kZv9rmOuuNrOz9vV1REpNwS8iEjMKfhGRmFHwy6gXDrF8wcxeNLMuM7vNzCaY2e/MbJuZPWJmYwat/5dm9rKZtZvZY2Z21KDnjjezJeF2Pwcqdnmvj5nZ0nDbRWZ23F7WfK2ZvWFm75jZA2Y2KVxuZvYNM9tkZlvDNs0Kn1tgZq+Eta0zs8/v1X+YxJ6CX6LiIuDDwPuAc4HfAV8ExhH8nn8awMzeB9wFfAZoAh4Efm1mGTPLAL8CfgyMBX4Zvi7htu8Hbgc+CTQCPwAeMLPsSAo1szOArwKXAhOBNcDPwqc/ApwWtqMBuAxoC5+7Dfiku9cCs4A/juR9Rfop+CUq/s3dN7r7OuBPwNPu/ry79wL3AceH610G/NbdH3b3HHAzUAl8EJgPpIFvunvO3e8Gnh30HtcCP3D3p9294O53AL3hdiPxceB2d18S1ncTcJKZTQNyQC1wJGDuvtzdN4Tb5YCjzazO3be4+5IRvq8IoOCX6Ng46H73EI9rwvuTCHrYALh7EVgLNIfPrfOdz1y4ZtD9Q4HPhcM87WbWDkwJtxuJXWvoJOjVN7v7H4HvAN8FNprZQjOrC1e9CFgArDGzx83spBG+rwig4Jf4WU8Q4EAwpk4Q3uuADUBzuKzf1EH31wL/290bBv2rcve79rGGaoKho3UA7v5td58LHEMw5POFcPmz7n4eMJ5gSOoXI3xfEUDBL/HzC+AcMzvTzNLA5wiGaxYBTwJ54NNmljKzC4ETB217K/C3ZvaB8CBstZmdY2a1I6zhTuBqM5sTHh/4CsHQ1GozOyF8/TTQBfQAhfAYxMfNrD4couoACvvw/yAxpuCXWHH314ArgH8DNhMcCD7X3fvcvQ+4EPgEsIXgeMC9g7ZdTDDO/53w+TfCdUdaw6PAPwL3EHzKOAy4PHy6jmAHs4VgOKiN4DgEwJXAajPrAP42bIfIiJkuxCIiEi/q8YuIxIyCX0QkZhT8IiIxo+AXEYmZVLkLGI5x48b5tGnTyl2GiMio8txzz21296Zdl4+K4J82bRqLFy8udxkiIqOKma0ZarmGekREYkbBLyISMwp+EZGYGRVj/CIiI5XL5WhpaaGnp6fcpZRcRUUFkydPJp1OD2t9Bb+IRFJLSwu1tbVMmzaNnU+4Gi3uTltbGy0tLUyfPn1Y22ioR0Qiqaenh8bGxkiHPoCZ0djYOKJPNgp+EYmsqId+v5G2M9LB/+jyjdzy2BvlLkNE5KAS6eB//PVWFj6xqtxliEhMtbe3c8stt4x4uwULFtDe3r7/CwpFOvgzyQR9+WK5yxCRmNpd8BcKe7542oMPPkhDQ0OJqor4rJ5MKkGvgl9EyuTGG29k5cqVzJkzh3Q6TU1NDRMnTmTp0qW88sornH/++axdu5aenh5uuOEGrrvuOmDHaWo6Ozs5++yzOeWUU1i0aBHNzc3cf//9VFZW7lNdkQ7+bCpJoegUik4yEY+DPCLybv/865d5ZX3Hfn3NoyfV8eVzj9njOl/72tdYtmwZS5cu5bHHHuOcc85h2bJlA9Mub7/9dsaOHUt3dzcnnHACF110EY2NjTu9xooVK7jrrru49dZbufTSS7nnnnu44op9u+pmtId6UkHzNNwjIgeDE088cae59t/+9reZPXs28+fPZ+3ataxYseJd20yfPp05c+YAMHfuXFavXr3PdUS6xz84+CszyTJXIyLl8l498wOlurp64P5jjz3GI488wpNPPklVVRWnn376kHPxs9nswP1kMkl3d/c+1xGLHn/vexxIEREphdraWrZt2zbkc1u3bmXMmDFUVVXx6quv8tRTTx2wuiLd488mNdQjIuXT2NjIySefzKxZs6isrGTChAkDz330ox/l+9//PscddxwzZ85k/vz5B6yukga/md0AXAsYcKu7f9PM/ilc1hqu9kV3f7AU7z/Q41fwi0iZ3HnnnUMuz2az/O53vxvyuf5x/HHjxrFs2bKB5Z///Of3S00lC34zm0UQ8CcCfcBDZvbb8OlvuPvNpXrvfjq4KyLybqXs8R8FPOXu2wHM7HHgghK+37tkFfwiIu9SyoO7y4DTzKzRzKqABcCU8LlPmdmLZna7mY0pVQEDPf6Cgl8kjty93CUcECNtZ8mC392XA/8KPAw8BLwA5IHvAYcBc4ANwNeH2t7MrjOzxWa2uLW1dahV3lNGB3dFYquiooK2trbIh3//+fgrKiqGvU1JD+66+23AbQBm9hWgxd039j9vZrcCv9nNtguBhQDz5s3bq5+cxvhF4mvy5Mm0tLSwtx3H0aT/ClzDVepZPePdfZOZTQUuBE4ys4nuviFc5QKCIaGS0KwekfhKp9PDviJV3JR6Hv89ZtYI5IDr3X2Lmf3YzOYADqwGPlmqN88OBL++wCUi0q/UQz2nDrHsylK+52DZVHCaBg31iIjsEItTNmhWj4jIDtEOfs3qERF5l2gHv2b1iIi8i4JfRCRmIh38qYRhpumcIiKDRTr4zSy44LoO7oqIDIh08EMwl19DPSIiO0Q++DOppIZ6REQGiXzwq8cvIrKzyAd/JqUxfhGRwaIf/MkEfTpXj4jIgOgHfyqhMX4RkUEiH/wa4xcR2Vnkgz+j4BcR2Uk8gl8Hd0VEBkQ/+JPq8YuIDBb94NdQj4jITmIR/JrVIyKyQ+SDP6vgFxHZSQyCP6kvcImIDBL54NesHhGRnUU/+DWrR0RkJ9EP/lSCokNevX4RESAmwQ+6/KKISL/oB39SF1wXERks8sGfTYfBr6EeEREgBsGvHr+IyM6iH/wa4xcR2Unkgz+bUo9fRGSwyAd/f49fY/wiIoHoB38yCUBvTqdtEBGBOAS/evwiIjuJfPBrjF9EZGeRD/6Mgl9EZCfxCX4N9YiIACUOfjO7wcyWmdnLZvaZcNlYM3vYzFaEt2NKWUP/F7g0j19EJFCy4DezWcC1wInAbOBjZnYEcCPwqLsfATwaPi6ZrL7AJSKyk1L2+I8CnnL37e6eBx4HLgDOA+4I17kDOL+ENWiMX0RkF6UM/mXAaWbWaGZVwAJgCjDB3TcAhLfjh9rYzK4zs8Vmtri1tXWvi8imgnn8Cn4RkUDJgt/dlwP/CjwMPAS8AORHsP1Cd5/n7vOampr2ug71+EVEdlbSg7vufpu7v9/dTwPeAVYAG81sIkB4u6mUNSQTRjJh9BX0zV0RESj9rJ7x4e1U4ELgLuAB4KpwlauA+0tZA+i6uyIig6VK/Pr3mFkjkAOud/ctZvY14Bdmdg3wFnBJiWsgk1Lwi4j0K2nwu/upQyxrA84s5fvuKpNKaDqniEgo8t/cBQ31iIgMFovgz6YT9OqUDSIiQEyCXz1+EZEdYhH8WR3cFREZEIvg16weEZEdYhP8vXl9gUtEBOIS/MmEzscvIhKKRfBnU0kN9YiIhGIR/BrjFxHZQcEvIhIz8Ql+jfGLiABxCf6kztUjItIvFsGf1UnaREQGxCL4+8f43b3cpYiIlF0sgj8bXn4xV1Dwi4jEIvgHrrurA7wiIjEJ/qQuuC4i0i8ewZ9KAgp+ERGITfAHzdSJ2kREYhb86vGLiMQk+LMDPX4Fv4hILIJfs3pERHaIRfBnNatHRGRALIJfY/wiIjvEKvg1xi8iErPgV49fRCQuwd8/xl/QPH4RkVgEfzatb+6KiPSLRfDrXD0iIjvEI/h1cFdEZEAsgj+rL3CJiAyIRfD3D/X05hT8IiLDCn4zu8HM6ixwm5ktMbOPlLq4/SWRMFIJU49fRITh9/j/2t07gI8ATcDVwNdKVlUJZMPr7oqIxN1wg9/C2wXAD939hUHLdr+R2WfN7GUzW2Zmd5lZhZn9k5mtM7Ol4b8Fe1v8SGQU/CIiAKSGud5zZvYHYDpwk5nVAntMUTNrBj4NHO3u3Wb2C+Dy8OlvuPvNe1v03lDwi4gEhhv81wBzgFXuvt3MxhIM9wzn9SvNLAdUAeuBaXtR5z7LpBIa4xcRYfhDPScBr7l7u5ldAfwDsHVPG7j7OuBm4C1gA7DV3f8QPv0pM3vRzG43szFDbW9m15nZYjNb3NraOswydy+TTOjSiyIiDD/4vwdsN7PZwN8Da4D/2NMGYaCfRzA8NAmoDnca3wMOI/gEsQH4+lDbu/tCd5/n7vOampqGWebuZVJJDfWIiDD84M+7uxME+bfc/VtA7Xtscxbwpru3unsOuBf4oLtvdPeCuxeBW4ET97b4kcikEvrmrogIww/+bWZ2E3Al8FszSwLp99jmLWC+mVWZmQFnAsvNbOKgdS4Alo206L2h6ZwiIoHhBv9lQC/BfP63gWbg/+5pA3d/GrgbWAK8FL7XQuD/mNlLZvYi8BfAZ/ey9hHJ6uCuiAgwzFk97v62mf0UOMHMPgY84+57HOMPt/sy8OVdFl858jL3XSapHr+ICAz/lA2XAs8AlwCXAk+b2cWlLGx/0zx+EZHAcOfxfwk4wd03AZhZE/AIwVDOqKCDuyIigeGO8Sf6Qz/UNoJtDwoa6hERCQy3x/+Qmf0euCt8fBnwYGlKKg19c1dEJDDcg7tfMLOLgJMJTs620N3vK2ll+1lWX+ASEQGG3+PH3e8B7ilhLSWlg7siIoE9Br+ZbQN8qKcAd/e6klRVArUVKfoKRbp681Rnh72/ExGJnD0moLu/12kZRo3p46oBWN3WxTGT6stcjYhI+YyqmTn7YlpjEPxvbu4qcyUiIuUVn+AfVwXAm60KfhGJt9gEf1UmxcT6Ct5sU/CLSLzFJvghGO7RUI+IxF2sgn96k4JfRCRWwT9jXDXt23Ns6eordykiImUTq+Dvn9KpcX4RibNYBf+0/uDXzB4RibFYBf+UMVUkE8Zq9fhFJMZiFfyZVIIpYypZpQO8IhJjsQp+CIZ7NNQjInEWu+CfPq6a1W1duA917jkRkeiLXfDPGFfN9r4Cm7b1lrsUEZGyiF3wTx9XA8AqDfeISEzFLvj7T9ammT0iElexC/5J9ZVkUgmdukFEYit2wZ9IGNMbqzXUIyKxFbvgh2C4R0M9IhJXsQz+6eNqWNPWRb6gi6+LSPzEMviPGF9DruDq9YtILMUy+I+dHFxs/cWWrWWuRETkwItl8B/WVENlOslL6xT8IhI/sQz+ZMI4elIdyxT8IhJDsQx+gGOb63l5fQeFos7ZIyLxEtvgn9Vcz/a+AqtaO8tdiojIARXb4D8uPMCrcX4RiZuSBr+ZfdbMXjazZWZ2l5lVmNlYM3vYzFaEt2NKWcPu6ACviMRVyYLfzJqBTwPz3H0WkAQuB24EHnX3I4BHw8cHnA7wikhclXqoJwVUmlkKqALWA+cBd4TP3wGcX+IaduvY5nqWrdMBXhGJl5IFv7uvA24G3gI2AFvd/Q/ABHffEK6zARg/1PZmdp2ZLTazxa2trSWp8djmerpzOsArIvFSyqGeMQS9++nAJKDazK4Y7vbuvtDd57n7vKamppLUeKwO8IpIDJVyqOcs4E13b3X3HHAv8EFgo5lNBAhvN5Wwhj3SAV4RiaNSBv9bwHwzqzIzA84ElgMPAFeF61wF3F/CGvao/wDvSzpnj4jESKpUL+zuT5vZ3cASIA88DywEaoBfmNk1BDuHS0pVw3Ac21zPz59dS6HoJBNWzlJERA6Iks7qcfcvu/uR7j7L3a909153b3P3M939iPD2nVLW8F5mTwkO8Gq4R0TiIrbf3O13xpETyKQS/Or5deUuRUTkgIh98NdXpjnrqPH8+oX15HRFLhGJgdgHP8AFx0+mrauPP60ozfcFREQOJgp+4EPva2JMVZp7l2i4R0SiT8EPZFIJzp09iYdf2UhHT67c5YiIlJSCP3TB8c305os89NLb5S5FRKSkFPyhOVMamD6umnufbyl3KSIiJaXgD5kZFxzfzFOr3mFde3e5yxERKRkF/yAXHN8MwH1L1OsXkehS8A8yZWwV82eM5e7nWnDXOfpFJJoU/Lu4ZO4UVrdtZ/GaLeUuRUSkJBT8uzj72EOoziS5e7GGe0QkmhT8u6jKpFhw7ER++9IGtvfly12OiMh+p+AfwsVzJ9PZm+f3L2tOv4hEj4J/CCdOH8vUsVX8UsM9IhJBCv4hmBkXz53MopVtrH1ne7nLERHZrxT8u3HR3MkkE8ZPnlpT7lJERPYrBf9uNDdUcvasQ7jzmbfo7NVBXhGJDgX/HvzNqTPY1pPn58+uLXcpIiL7jYJ/D+ZMaeCEaWO4/c9vktfVuUQkIhT87+FvTp3BuvZuHtLUThGJCAX/ezjrqAlMa6zi1j+9qfP3iEgkKPjfQzJhXHPKdF5Y284TKzaXuxwRkX2m4B+Gi+dO4bCmam742fO8ubmr3OWIiOwTBf8wVGaS3P6JEzDgmh89y9btui6viIxeCv5hOrSxmoX/bR4tW7r5u58+R06zfERklFLwj8AJ08by1QuPZdHKNq66/Rk2bespd0kiIiOm4B+hi+ZO5uZLZrPkrS0s+NafWfSGDviKyOii4N8LF8+dzP3Xn0J9ZYorbnuarzy4nC6d1kFERgkF/16aeUgtD3zqFC47YQoLn1jFmV9/nN+8uF5z/UXkoKfg3wfV2RRfvfA47vm7DzK2OsOn7nyei7//JI+9tkk7ABE5aNloCKh58+b54sWLy13GHhWKzl3PvMUt//kG67f2cGxzPdecMp0PHz2B6myq3OWJSAyZ2XPuPu9dyxX8+1dfvsi9S1q45bGVvPXOdirTST589AQWHDuRkw9vpLYiXe4SRSQmFPwHWLHoPLv6He5/YT0PvrSB9u05Uglj7qFjOO19TZx8+DhmTaojldRom4iUxgEPfjObCfx80KIZwP8AGoBrgdZw+Rfd/cE9vdZoDP7BcoUiz63ZwuOvt/LYa60s39ABQG02xfzDGjnjyPGcceR4JtRVlLlSEYmSsvb4zSwJrAM+AFwNdLr7zcPdfrQH/642d/by5Mo2Fq3czBOvb2ZdezcAs6c0cMUHpnLu7ElUpJNlrlJERrvdBf+BOup4JrDS3deY2QF6y4PXuJos586exLmzJ+HuvL6xk0eWb+RXz6/jC3e/yFceXM5ffWAq1546g4aqTLnLFZGIOVA9/tuBJe7+HTP7J+ATQAewGPicu28ZYpvrgOsApk6dOnfNmuhf9NzdeXJlGz9atJqHl2+kJpPikx+awdUnT9fMIBEZsbIN9ZhZBlgPHOPuG81sArAZcOB/AhPd/a/39BpRG+oZjlff7uDm37/OI8s3MrY6w4XHN3PxvMkceUhduUsTkVGinMF/HnC9u39kiOemAb9x91l7eo04Bn+/JW9tYeHjq3j01Y3kCs4xk+o466gJnD6zieMmN5BMaOhMRIZWzuD/GfB7d/9h+Hiiu28I738W+IC7X76n14hz8Pd7p6uP+5eu44EX1rN0bTvuMKYqzfunjmH2lAZmT2ng8PE1TKyrIKGdgYhQpuA3sypgLTDD3beGy34MzCEY6lkNfLJ/R7A7Cv6dvdPVx59WtPKnFZtZuradla2d9P8Ys6kEhzZWMbG+kkPqKphQl2V8XQXja7NMqKtgXG2WxuqMZg2JxIC+wBVhHT05Xl7XwarNnaze3MXqtu28vbWHjR09bO7spTjEj7gmm2J8bZZD6is4pK6CiQ0VNDdUMamhguaGSiY1VOqAssgoV+7pnFJCdRVpTjqskZMOa3zXc/lCkbauPjZ29LCpo5e2rl42d/axubOXTdt6eXtrD0+/+Q5vd/RQ2GUPUV+ZZsrYSo4YX8vh42uYOaGWOVMbGFeTPVBNE5ESUPBHXCqZYEJdxXt+K7hQdDZt62Hdlm7WtXezvr2Hde3bWdO2nadWtXHf8+sG1p06torjpzYw79AxzD10LDMPqdVBZpFRRMEvACQTxsT6SibWV/Kuz4VAZ2+eV9Z3sHTtFpasaWfRyjbuX7oegNqKFKcd0cTpM5v40Mwmxtfq1BMiBzMFvwxLTTbFidPHcuL0sUDwZbOWLd08t2YLi1Zu5rHXWvntS8Ex+jlTGvjw0RM448jxzJxQq1lGIgcZHdyV/cLdeWVDB39cvolHlm/khZatQHAiulnN9Rw3pZ6jDqlj5iG1zGiqJpvSrCI5eBSLTk++wOA4bOvsY/3WbjZ29NDZmyeXL5IrOH2FIoWiky8USSSMbCpJRTpBKmFghhGcmHFbT55tPTlyBaeuMk1dRYpsOklfvkhvvkBPrkh3X56uvgK5fJGqTJKqbIqabIrG6gyNNVnG1WQ4fHzNXp/OXbN65IDa2NHDE6+38kJLOy+2bGX5hg5yheB3LZkwxtcG00wn1GaZWF/BhPoKJtZX0FCZoSqTpDqbYkx1RlNPR4l8oUhfoUguHwRjb75Ab75IX76IGSTNSCQsDFbHHXpyRbr68mzvy1MoggGJBOQLPrCtA+mkkQy37c4V6MkV6AtDuFAski96EMRFpy9fpLMnT2dv8K87V6C7r0BvvkDRoehOsegDAd6XL9KTC2othcp0klTC2Laba3JXppNUZZJkUgm6cwW6evMDfyf9fnj1CfzFzPF79f4KfimrvnyR1W1dvPb2NlZs3Mb6cLrpxo4e3t7aQ0fP7i9WX5NNUV+ZpiaboqYiRVUmSUU6SWX4rzqboiabpKYiFd5PUZ1JUZFOkk0nyKYSJBNGKpEgmYBCMQwAd5IJGwilTDJBJpUgnUyQShrpRHCbGHRiwaBTd3APXbkHwdabDwKyP+ByhSK9YTC2d+fYuj1Hb6FIKvw/yBWDXmpHd46eXBCEZuDOQMD25cOebG+Oju48W7b3saWrj66+QlnbbAaphJFOJqitSIW/K2kq0wkq00myqSTJhGHhzy/4WQe3FZng96ginSQRthdgTHWGSfWVHFJfQV1FasfvRTLo3ScTRtEZ2Hnki47j4MGkitpwGwgmT3T25unNFcimgt/LTDIx5DBoT65AW1cfm7f1srmzlzlTGmjcy5l0Cn45qG3vyw/sAIIeW4727bngD6Czl47uYFlnb57tfcHH5J5cge19ebp6C3Tupke1v5lBOhn80SYTRsIgYTZoZxD0ZoMdS3DrzpDXYDYLggiCoYZCuE3SjFQi2Bn1bw/Bl/P6d2b9vdZcoTjwXoUwmHcdshipVMLe9SkrlbSBYK3JpqitSFFXmWZMVYYxVRnqK9NUpIOdZjoV7GyzqeD/yQlqK4ZFWTgcUpFOUp1JUplJDvToPXz/bCrYCRtGPtzp9G9TGfaQ04n+HbrpONJuaB6/HNSqMilmNNXs9fbFotM1aCfQ1ZunNxxL7c0VKXgwHFAoOgkzgo6Y4e4DQwW5QjiGmw96b7lCMI7bn6H9Pd/ecEij/1NDoegMztn+nUH/J4VEGPCDo8nD1/NwR5EMe5Bm4U6gGLyXha/jBKHeHQ5LJC0I4XQyCL2kBTuhTP/OIZUgm0oOfILpD9JMKjHwCaqhKk0mlaBYhHyxSDJh1FemqUwnD/pPNbJvFPwSCYmEUVuR1jWNRYZBF3wVEYkZBb+ISMwo+EVEYkbBLyISMwp+EZGYUfCLiMSMgl9EJGYU/CIiMTMqTtlgZq3Amr3cfByweT+WM1rEsd1xbDPEs91xbDOMvN2HunvTrgtHRfDvCzNbPNS5KqIuju2OY5shnu2OY5th/7VbQz0iIjGj4BcRiZk4BP/CchdQJnFsdxzbDPFsdxzbDPup3ZEf4xcRkZ3FoccvIiKDKPhFRGIm0sFvZh81s9fM7A0zu7Hc9ZSCmU0xs/80s+Vm9rKZ3RAuH2tmD5vZivB2TLlr3d/MLGlmz5vZb8LHcWhzg5ndbWavhj/zk6LebjP7bPi7vczM7jKziii22cxuN7NNZrZs0LLdttPMbgqz7TUz+y8jea/IBr+ZJYHvAmcDRwP/1cyOLm9VJZEHPufuRwHzgevDdt4IPOruRwCPho+j5gZg+aDHcWjzt4CH3P1IYDZB+yPbbjNrBj4NzHP3WUASuJxotvlHwEd3WTZkO8O/8cuBY8Jtbgkzb1giG/zAicAb7r7K3fuAnwHnlbmm/c7dN7j7kvD+NoIgaCZo6x3hancA55elwBIxs8nAOcC/D1oc9TbXAacBtwG4e5+7txPxdhNcIrbSzFJAFbCeCLbZ3Z8A3tll8e7aeR7wM3fvdfc3gTcIMm9Yohz8zcDaQY9bwmWRZWbTgOOBp4EJ7r4Bgp0DML6MpZXCN4G/B4qDlkW9zTOAVuCH4RDXv5tZNRFut7uvA24G3gI2AFvd/Q9EuM272F079ynfohz8NsSyyM5dNbMa4B7gM+7eUe56SsnMPgZscvfnyl3LAZYC3g98z92PB7qIxhDHboVj2ucB04FJQLWZXVHeqg4K+5RvUQ7+FmDKoMeTCT4iRo6ZpQlC/6fufm+4eKOZTQyfnwhsKld9JXAy8JdmtppgCO8MM/sJ0W4zBL/TLe7+dPj4boIdQZTbfRbwpru3unsOuBf4INFu82C7a+c+5VuUg/9Z4Agzm25mGYIDIQ+Uuab9zsyMYMx3ubv/v0FPPQBcFd6/Crj/QNdWKu5+k7tPdvdpBD/XP7r7FUS4zQDu/jaw1sxmhovOBF4h2u1+C5hvZlXh7/qZBMexotzmwXbXzgeAy80sa2bTgSOAZ4b9qu4e2X/AAuB1YCXwpXLXU6I2nkLwEe9FYGn4bwHQSDALYEV4O7bctZao/acDvwnvR77NwBxgcfjz/hUwJurtBv4ZeBVYBvwYyEaxzcBdBMcxcgQ9+mv21E7gS2G2vQacPZL30ikbRERiJspDPSIiMgQFv4hIzCj4RURiRsEvIhIzCn4RkZhR8IuUmJmd3n8GUZGDgYJfRCRmFPwiITO7wsyeMbOlZvaD8Hz/nWb2dTNbYmaPmllTuO4cM3vKzF40s/v6z5NuZoeb2SNm9kK4zWHhy9cMOo/+T8NvoYqUhYJfBDCzo4DLgJPdfQ5QAD4OVANL3P39wOPAl8NN/gP47+5+HPDSoOU/Bb7r7rMJzimzIVx+PPAZgmtDzCA435BIWaTKXYDIQeJMYC7wbNgZryQ4IVYR+Hm4zk+Ae82sHmhw98fD5XcAvzSzWqDZ3e8DcPcegPD1nnH3lvDxUmAa8OeSt0pkCAp+kYABd7j7TTstNPvHXdbb0zlO9jR80zvofgH97UkZaahHJPAocLGZjYeBa50eSvA3cnG4zl8Bf3b3rcAWMzs1XH4l8LgH10FoMbPzw9fImlnVgWyEyHCo1yECuPsrZvYPwB/MLEFwhsTrCS52coyZPQdsJTgOAMEpcr8fBvsq4Opw+ZXAD8zsX8LXuOQANkNkWHR2TpE9MLNOd68pdx0i+5OGekREYkY9fhGRmFGPX0QkZhT8IiIxo+AXEYkZBb+ISMwo+EVEYub/AyHkdt41KRzRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results.history['loss'])\n",
    "#plt.plot(results.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manero/anaconda3/lib/python3.8/site-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    }
   ],
   "source": [
    "# Prediction of training vector\n",
    "X_train_pred = vae_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_vector = get_error_term(X_train_pred, X_train, _rmse=False)\n",
    "print(f'Avg error {np.mean(mae_vector)}\\nmedian error \\\n",
    "      {np.median(mae_vector)}\\n99Q: {np.quantile(mae_vector, 0.99)}')\n",
    "\n",
    "error_thresh = np.quantile(mae_vector, 0.99)\n",
    "print('error threshold >>>', error_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# application of error threshold in test vector\n",
    "# A vector of anomalies is created with the points that go over the threshold\n",
    "\n",
    "X_pred = vae_model.predict(X_test)\n",
    "mae_vector = get_error_term(X_pred, X_test, _rmse=False)\n",
    "anomalies = (mae_vector > error_thresh)\n",
    "\n",
    "np.count_nonzero(anomalies) / len(anomalies)\n",
    "print('total length vector anomalies : ',len(anomalies))\n",
    "print('Number of anomalies : ', np.count_nonzero(anomalies))\n",
    "print('total length of y_test vector',len(y_test))\n",
    "print('total lenght of attacks in y_test vector', np.count_nonzero(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn classification report\n",
    "print(classification_report(y_test, anomalies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second prediction with X_test vector to be used in PCA transformation\n",
    "X_pred2 = encoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA transformation to observe variance in 2 components\n",
    "\n",
    "\n",
    "#, svd_solver='arpack'\n",
    "pca = PCA(n_components=2, random_state=123)\n",
    "X_transform = pca.fit_transform(X_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure comparison PCA transformations\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(18,4))\n",
    "fig.suptitle('PCA transformations comparison',size=20)\n",
    "sns.scatterplot(x=X_transform[:, 0], y=X_transform[:, 1], s=20, hue=mae_vector, ax=ax1)\n",
    "ax1.set_xlabel('MAE Vector', size=16)\n",
    "sns.scatterplot(x=X_transform[:, 0], y=X_transform[:, 1], s=20, hue=anomalies, ax=ax2)\n",
    "ax2.set_xlabel('Anomalies', size = 16)\n",
    "legend_labels2, _ = ax2.get_legend_handles_labels()\n",
    "ax2.legend(legend_labels2, ['Normal', 'Anomaly'], title='Anomalies')\n",
    "sns.scatterplot(x=X_transform[:, 0], y=X_transform[:, 1], s=10, hue=y_test, ax=ax3)\n",
    "ax3.set_xlabel('y_test', size = 16)\n",
    "legend_labels3, _ = ax3.get_legend_handles_labels()\n",
    "ax3.legend(legend_labels3, ['Normal', 'Attack'], title='Traffic')\n",
    "plt.xlim([-10, 10])\n",
    "plt.ylim([-10, 10])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12,10))\n",
    "ax.set_title('PCA transformation comparison \\n MAE Vector',size=20)\n",
    "ax.set_ylabel('Component 2', size=16)\n",
    "ax.set_xlabel('Component 1', size=16)\n",
    "sns.scatterplot(x=X_transform[:, 0], y=X_transform[:, 1], s=20, hue=mae_vector)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12,10))\n",
    "ax.set_title('PCA transformation comparison \\n Anomalies',size=20)\n",
    "ax.set_ylabel('Component 2', size=16)\n",
    "ax.set_xlabel('Component 1', size=16)\n",
    "sns.scatterplot(x=X_transform[:, 0], y=X_transform[:, 1], s=20, hue=anomalies)\n",
    "legend_label, _ = ax.get_legend_handles_labels()\n",
    "ax.legend(legend_label, ['Normal', 'Anomaly'], title='Anomalies')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12,10))\n",
    "ax.set_title('PCA transformation comparison \\n Traffic Classification',size=20)\n",
    "ax.set_ylabel('Component 2', size=16)\n",
    "ax.set_xlabel('Component 1', size=16)\n",
    "sns.scatterplot(x=X_transform[:, 0], y=X_transform[:, 1], s=10, hue=y_test)\n",
    "legend_label, _ = ax.get_legend_handles_labels()\n",
    "ax.legend(legend_label, ['Normal', 'Attack'], title='Traffic')\n",
    "plt.xlim([-10, 10])\n",
    "plt.ylim([-10, 10])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction figure\n",
    "plt.rc('legend',fontsize=14)\n",
    "error_df = pd.DataFrame({'Reconstruction_error': np.squeeze(mae_vector),\n",
    "                        'True_class': y_test})\n",
    "\n",
    "error_df = error_df.sample(frac=1).reset_index(drop=True)\n",
    "threshold_fixed = error_thresh\n",
    "groups = error_df.groupby('True_class')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,6))\n",
    "for name, group in groups:\n",
    "    ax.plot(group.index, group.Reconstruction_error, marker='o', ms=1, linestyle='',\n",
    "            label= \"Attack\" if name == 1 else \"Normal\")\n",
    "\n",
    "ax.hlines(threshold_fixed, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", \n",
    "          zorder=100, label='Threshold')\n",
    "plt.xlim(0,error_df.shape[0])\n",
    "# leg = ax.legend()\n",
    "# leg.set_title('Traffic',prop={'size':14})\n",
    "plt.legend(legend_label, ['Normal', 'Attack'], prop={'size': 14}, \n",
    "          loc=\"upper right\", markerscale=2., scatterpoints=1)\n",
    "plt.title(\"Reconstruction error for normal and attack data\", size=18)\n",
    "plt.ylabel(\"Reconstruction error\", size=16)\n",
    "plt.xlabel(\"Data point index\", size = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction errors for figure\n",
    "\n",
    "Global_error = np.squeeze(mae_vector)\n",
    "Attack_error = error_df[error_df['True_class']==1].Reconstruction_error.to_numpy()\n",
    "Normal_error = error_df[error_df['True_class']==0].Reconstruction_error.to_numpy()\n",
    "print(' Attack_error shape', Attack_error.shape, '\\n', \n",
    "      'Normal_error shape', Normal_error.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure error distribution\n",
    "\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(18,6))\n",
    "fig.suptitle('Error distribution comparison between different traffic categories',\n",
    "             size=20)\n",
    "ax1.hist(Normal_error, bins=150, color='green', alpha=0.6)\n",
    "ax1.set_xlabel('Normal error', size=14)\n",
    "ax1.set_ylabel('No of samples', size = 14)\n",
    "ax2.hist(Attack_error, bins=150, color='orange')\n",
    "ax2.set_xlabel('Attack error', size = 14)\n",
    "ax2.set_ylabel('No of samples', size = 14)\n",
    "ax3.hist(Attack_error, bins=150, color='orange')\n",
    "ax3.hist(Normal_error, bins=150, color='green', alpha=0.6)\n",
    "ax3.set_xlabel('Normal and attack', size = 14)\n",
    "ax3.set_ylabel('No of samples', size = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paper Figure ===\n",
    "# Distribución reconstruction error \n",
    "latent_space = latent_dim\n",
    "plt.rcParams['axes.titley'] = 1.12\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 6))\n",
    "sns.kdeplot(\n",
    "    Normal_error,\n",
    "    fill    = True,\n",
    "    ax      = ax,\n",
    "    label   = 'Normal',\n",
    "    color   = 'blue'\n",
    ")\n",
    "sns.kdeplot(\n",
    "    Attack_error,\n",
    "    fill    = True,\n",
    "    ax      = ax,\n",
    "    label   = 'Attack',\n",
    "    color   = 'red'\n",
    ")\n",
    "#sns.rugplot(error_reconstruccion,  ax=ax, color='black')\n",
    "plt.suptitle('Distribution Reconstruction Error',y=0.92, \n",
    "            fontsize=16)\n",
    "ax.set_title('VAE Autoencoder latent space '+str(latent_space)+' cells', fontsize=15)\n",
    "ax.set_xlabel('Reconstruction error', fontsize = 14);\n",
    "ax.set_ylabel('Density',fontsize=14);\n",
    "ax.set_xlim(0,0.25)\n",
    "plt.legend( fontsize=16)\n",
    "plt.savefig(path_figures+'VAE_'+str(latent_space)+'_rea_ele.png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisis latent space with t-SNE\n",
    "\n",
    "plt.rcParams['axes.titley'] = 1.05\n",
    "tsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=300, init='pca', random_state=123)\n",
    "tsne_results = tsne.fit_transform(X_pred2)\n",
    "\n",
    "df_subset=pd.DataFrame(tsne_results, columns=['tsne-2d-one','tsne-2d-two']) \n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,10))\n",
    "ax.set_title('t-sne applied to latent space \\n Traffic Classification',size=20)\n",
    "ax.set_ylabel('Component 2', size=16)\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"\", [\"blue\",\"orange\"])\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "    hue=y_test,\n",
    "    alpha=1,\n",
    "    data=df_subset,\n",
    "    cmap=custom_cmap,\n",
    "    s = 8\n",
    ")\n",
    "ax.set_xlabel('Component 1', size=16)\n",
    "legend_label, _ = ax.get_legend_handles_labels()\n",
    "ax.legend(legend_label, ['Normal', 'Attack'], title='Traffic', fontsize=12, title_fontsize=12)\n",
    "ax.set_xlim(-2.5,2.5)\n",
    "ax.set_ylim(-2.5,2.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-sne transformation to observe variance in 3 components of latent space\n",
    "\n",
    "tsne = TSNE(n_components=3, verbose=0, perplexity=40, n_iter=300, init='pca', random_state=123)\n",
    "tsne_results = tsne.fit_transform(X_pred2)\n",
    "plt.rcParams['axes.titlepad'] = -14 \n",
    "df_subset=pd.DataFrame(tsne_results, columns=['tsne-2d-one','tsne-2d-two', 'tsne-2d-three']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-sne figure 3D\n",
    "\n",
    "plt.rcParams['axes.titley'] = .95\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax= Axes3D(fig, auto_add_to_figure=False)\n",
    "fig.add_axes(ax)\n",
    "ax.axes.set_xlim3d(left=-4, right=6)\n",
    "ax.axes.set_ylim3d(bottom=-4, top=6)\n",
    "ax.axes.set_zlim3d(bottom=-3, top=3)\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"\", [\"blue\",\"orange\"])\n",
    "sc = ax.scatter(tsne_results[:, 0], tsne_results[:, 1], tsne_results[:, 2],\n",
    "               c=y_test, alpha = 1, cmap=custom_cmap, s=2)\n",
    "plt.title('3D t-sne applied to latent space \\n Traffic Classification',size=20)\n",
    "\n",
    "ax.set_xlabel('Component 1', size=16)\n",
    "ax.set_ylabel('Component 2', size=16)\n",
    "ax.set_zlabel('Component 3', size=16)\n",
    "# rotate the axes and update\n",
    "#fig.tight_layout()\n",
    "fig.subplots_adjust(left=0, right=1, bottom=-10, top=-2)\n",
    "ax.view_init(10, 60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
